{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Developping a pipeline to normalize and clean the MRI images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# default_exp preprocessing\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import SimpleITK as sitk\n",
    "from faimed3d.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is stored on a secure network drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/media/ScaleOut/vahldiek/MRI/SIJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preferred structure of the dataframe changed during the project. So I need to adapt it a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR/'dataset_training_477.csv')\n",
    "df = df[df.has_TIRM_and_T1 == 1]\n",
    "df['path_TIRM'] = [fn.replace('../data/', '') for fn in df.path_TIRM]\n",
    "df['path_T1'] = [fn.replace('../data/', '') for fn in df.path_T1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "Using the DicomExplorer from `faimed3d` multiple images are viewed and analyzed. It seems, the pixel/voxel values have a strongly skewed distribution with most pixel-values beeing on the lower end of the histogram and a long tail of high intensities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = pd.concat([df[df.cohort == cohort].sample(5) for cohort in df.cohort.unique()]) # viewing a representative subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = DATA_DIR/'TRAINING/01012/T1' # large patient\n",
    "fn = DATA_DIR/'TRAINING/01093/T1/' # act. changes, struct changes, SPA and ASAS compatible\n",
    "#fn = DATA_DIR/'TRAINING/01084/TIRM/' # no act but struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics:\n",
      "  Mean px value:     500.3769226074219 \n",
      "  Std of px values:  473.8222961425781 \n",
      "  Min px value:      0.0 \n",
      "  Max px value:      2323.333251953125 \n",
      "  Median px value:   329.7845153808594 \n",
      "  Skewness:          1.2070366144180298 \n",
      "  Kurtosis:          0.37535691261291504 \n",
      "\n",
      "Tensor properties \n",
      "  Tensor shape:      (20, 224, 224)\n",
      "  Tensor dtype:      torch.float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec32c165d4df4d19ba59ca9f449d65db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(height='4.166666666666667in', width='4.166666666666667in'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = TensorDicom3D.create(fn)\n",
    "image = Resample3D((20, 224, 224), (3.5, 1, 1))(image)\n",
    "DicomExplorer(image, figsize = (5,5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values show a heavy right tailed distribution and different maximum intensitites.  \n",
    "Most images contain parts of the MRI table, the lower image areas are black, which explains part of the skewed distributions.  \n",
    "The image areas with very high intensities are probably from CSF.  \n",
    "This observation is true for all the 20 sampled images I viewed.  \n",
    "Clipping the lower range of the pixels seems to be no problem,as it only turns the very hypointense background to black. Clipping very high pixel intensities could also be save, but as active inflammation is also hyperintens, one needs to be carefull. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n",
    "### Field Bias Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def field_bias_correction(image, numberFittingLevels = 4, numberOfIteration = [50]):\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    corrector.SetMaximumNumberOfIterations(numberOfIteration * numberFittingLevels)\n",
    "    bias_corrected = corrector.Execute(\n",
    "        sitk.Cast(image.as_sitk(), \n",
    "                  sitk.sitkFloat32))\n",
    "    bias_corrected = TensorDicom3D.create(bias_corrected)\n",
    "    bias_corrected._metadata = image._metadata\n",
    "    return bias_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics:\n",
      "  Mean px value:     481.5676574707031 \n",
      "  Std of px values:  428.35113525390625 \n",
      "  Min px value:      0.0 \n",
      "  Max px value:      1889.69287109375 \n",
      "  Median px value:   331.3974914550781 \n",
      "  Skewness:          1.0523874759674072 \n",
      "  Kurtosis:          -0.09217572212219238 \n",
      "\n",
      "Tensor properties \n",
      "  Tensor shape:      (20, 224, 224)\n",
      "  Tensor dtype:      torch.float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f860e4f9b3af42e19871b86c4995848a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(height='4.166666666666667in', width='4.166666666666667in'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "bias_corrected_image = field_bias_correction(image)\n",
    "DicomExplorer(bias_corrected_image, figsize = (5,5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def denoising(image):\n",
    "    denoised_image = sitk.CurvatureFlow(\n",
    "        image1=image.as_sitk(),\n",
    "        timeStep=0.125,\n",
    "        numberOfIterations=3)\n",
    "    denoised_image = TensorDicom3D.create(denoised_image)\n",
    "    denoised_image._metadata = image._metadata\n",
    "    return denoised_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca0d8f978064f2f8982c4f83e802b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(height='4.166666666666667in', width='4.166666666666667in'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "denoised_image = denoising(image)\n",
    "DicomExplorer(denoised_image, figsize = (5,5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescaling(image, lwr=0, upr=255):\n",
    "    resacleFilter = sitk.RescaleIntensityImageFilter()\n",
    "    resacleFilter.SetOutputMaximum(upr)\n",
    "    resacleFilter.SetOutputMinimum(lwr)\n",
    "    scaled_image = resacleFilter.Execute(image.as_sitk())\n",
    "    return TensorDicom3D.create(scaled_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aac5fd0d2ab4d3eab76d13a9a7a39a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(height='4.166666666666667in', width='4.166666666666667in'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "scaled_image = rescaling(image)\n",
    "DicomExplorer(scaled_image, figsize = (5,5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping\n",
    "> Clip the tail of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clip_tail(image, lwr_quant=0, upr_quant=0.99):\n",
    "    lwr = image.quantile(lwr_quant)\n",
    "    upr = image.quantile(upr_quant)\n",
    "    return image.clip(lwr, upr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1106a637819e42b7927c3f69365de31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(height='4.166666666666667in', width='4.166666666666667in'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "clipped_image = clip_tail(image)\n",
    "DicomExplorer(clipped_image, figsize = (5,5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = TensorDicom3D.create(DATA_DIR/list(subset.path_T1)[0]).get_spacing()\n",
    "origin = TensorDicom3D.create(DATA_DIR/list(subset.path_T1)[0]).get_origin()\n",
    "direction = TensorDicom3D.create(DATA_DIR/list(subset.path_T1)[0]).get_direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "batch = torch.stack([Resample3D((20, 224, 224), (3.5, 1, 1))(TensorDicom3D.create(DATA_DIR/fn)) for fn in subset.path_T1])\n",
    "DicomExplorer(batch.mean(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "reference_image = batch.mean(0)\n",
    "reference_image.set_spacing(spacing)\n",
    "reference_image.set_direction(direction)\n",
    "reference_image.set_origin(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hist_matching(image, reference):\n",
    "    hist_matching = sitk.HistogramMatchingImageFilter()\n",
    "    hist_matching.ThresholdAtMeanIntensityOn()\n",
    "    hist_matched_image = hist_matching.Execute(image.as_sitk(), reference_image.as_sitk())\n",
    "    return TensorDicom3D.create(hist_matched_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "hist_matched_image = hist_matching(image, reference_image)\n",
    "DicomExplorer(hist_matched_image).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Pipeline():\n",
    "    def __init__(self, functions:(list, tuple)):\n",
    "        assert isinstance(functions, (list, tuple)), 'Functions need to be in a list or tuple'\n",
    "        self.functions = functions\n",
    "        \n",
    "    def append(self, f):\n",
    "        self.functions.append(f)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for f in self.functions: x = f(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([Resample3D((20, 224, 224), (3.5, 1, 1)), clip_tail, denoising, field_bias_correction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabled\n",
    "clipped_and_denoised_image = denoising(clipped_image)\n",
    "clipped_and_bias_corrected_image = field_bias_correction(clipped_image)\n",
    "clipped_and_denoised_and_bias_corrected = pipe(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabled\n",
    "clipped_and_denoised_and_bias_corrected_and_hist_matched = hist_matching(clipped_and_denoised_and_bias_corrected, reference_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabled\n",
    "images = [image, clipped_image, denoised_image, \n",
    "          bias_corrected_image, clipped_and_denoised_image, clipped_and_bias_corrected_image, \n",
    "          clipped_and_denoised_and_bias_corrected, clipped_and_denoised_and_bias_corrected_and_hist_matched]\n",
    "\n",
    "labels = ['original', 'clipped_image', 'denoised_image', \n",
    "          'bias_corrected_image', 'clipped_and_denoised_image', \n",
    "          'clipped_and_bias_corrected_image', 'clipped_and_denoised_and_bias_corrected', 'clipped_and_denoised_and_bias_corrected_and_hist_matched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabled\n",
    "DicomExplorer(clip_tail(clipped_and_denoised_and_bias_corrected, upr_quant=0.995),  cmap='Greys_r').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabled\n",
    "ListViewer(images, labels, cmap='Greys_r').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "preprocessing_pipeline = Pipeline([Resample3D((20, 224, 224), (3.5, 1, 1)), clip_tail, field_bias_correction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to loop through all images in the dataset, apply the preprocessing pipeline and then save the preprovessed image as NIfTI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faimed3d",
   "language": "python",
   "name": "faimed3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
